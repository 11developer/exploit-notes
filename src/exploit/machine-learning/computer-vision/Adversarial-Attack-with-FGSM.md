---
title: Adversarial Attack with FGSM (Fast Gradient Signed Method)
description: Adversarial Attack is the method to fool a neural network. This leads misclassification of a classification model.
tags: 
    - Computer Vision
    - Machine Learning
refs:
    - https://arxiv.org/abs/1412.6572
    - https://arxiv.org/abs/1810.00069
    - https://tcode2k16.github.io/blog/posts/picoctf-2018-writeup/general-skills/#solution-20
date: 2023-08-19
draft: false
---

## Create Adversarial Examples against MobileNetV2

Reference: [TensorFlow Docs](https://www.tensorflow.org/tutorials/generative/adversarial_fgsm)

### 1. Load Pretrained Model (MobileNetV2)

```python
import tensorflow as tf

pretrained_model = tf.keras.applications.MobileNetV2(include_top=True, weights='imagenet')
pretrained_model.trainable = False

# ImageNet labels
decode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions
```

### 2. Prepare Original Image

We create functions to preprocess image and get label at first.

```python
# Helper function to preprocess the image so that it can be inputted in MobileNetV2
def preprocess(image):
  image = tf.cast(image, tf.float32)
  image = tf.image.resize(image, (224, 224))
  image = tf.keras.applications.mobilenet_v2.preprocess_input(image)
  image = image[None, ...]
  return image

# Helper function to extract labels from probability vector
def get_imagenet_label(probs):
	return decode_predictions(probs, top=1)[0][0]
```

Then load the original image and preprocess it.

```python
orig_image_path = tf.keras.utils.get_file('YellowLabradorLooking_new.jpg', 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg')
orig_image_raw = tf.io.read_file(image_path)
orig_image = tf.image.decode_image(image_raw)

orig_image = preprocess(image)
orig_image_probs = pretrained_model.predict(image)
```

To get the label of the image that the model predicted, execute the following code.

```python
_, orig_image_class, orig_class_confidence = get_imagenet_label(orig_image_probs)

print(f"class: {orig_image_class}")
print(f"confidence: {orig_class_confidence}")

# The output
# class: Labrador_retriever
# confidence: 0.418184757232666
```

### 3. Create Adversarial Image with FGSM

From this, we create the adversarial image to fool the MobileNetV2 model. The following code creates the perturbations to modify the original image.

```python
# Instantiate a function that computes the crossentropy loss between labels and predictions.
loss_obj = tf.keras.losses.CategoricalCrossentropy()

def create_adversarial_pattern(input_image, input_label):
	# The gradient tape records the operations which are executed inside it.
  with tf.GradientTape() as tape:
    tape.watch(input_image)
    prediction = pretrained_model(input_image)
    loss = loss_obj(input_label, prediction)

  # Get the gradients of the loss w.r.t (with respect to) to the input image.
  gradient = tape.gradient(loss, input_image)
  # Get the sign of the gradients to create the perturbation.
  signed_grad = tf.sign(gradient)
  return signed_grad

# The index of the label for labrador retriever
target_label_idx = 208
orig_label = tf.one_hot(target_label_idx, orig_image_probs.shape[-1])
orig_label = tf.reshape(orig_label, (1, orig_image_probs.shape[-1]))

perturbations = create_adversarial_pattern(orig_image, orig_label)
```

Now create adversarial examples and predict the labels by the classification model while increasing epsilon.

```python
# Epsilons are error terms (very small numbers)
epsilons = [0, 0.01, 0.1, 0.15]

for i, eps in enumerate(epsilons):
	adv_image = orig_image + eps*perturbations
	adv_image = tf.clip_by_value(adv_image, -1, 1)
	# Predict the label and the confidence for the adversarial image
	_, label, confidence = get_imagenet_label(pretrained_model.predict(adv_image))
	print(f"predicted label: {label}")
	print(f"confidence: {confidence*100:.2f}%")
	print("-"*128)
```

The outputs are something like below.

```txt
1/1 [==============================] - 0s 25ms/step
predicted label: Labrador_retriever
confidence: 41.82%
--------------------------------------------------------------------------------------------------------------------------------
1/1 [==============================] - 0s 27ms/step
predicted label: Saluki
confidence: 13.08%
--------------------------------------------------------------------------------------------------------------------------------
1/1 [==============================] - 0s 24ms/step
predicted label: Weimaraner
confidence: 15.13%
--------------------------------------------------------------------------------------------------------------------------------
1/1 [==============================] - 0s 26ms/step
predicted label: Weimaraner
confidence: 16.58%
--------------------------------------------------------------------------------------------------------------------------------
```

As above, the adversarial examples were predicted as different labels from the label that the original image was predicted (the original label is labrador retriever).  
To display the final adversarial image, execute the following code.

```python
import matplotlib.pyplot as plt

plt.imshow(adv_image[0])
```

### 4. Save/Load the Adversarial Image

We can save the generated adversarial image as below.

```python
tf.keras.utils.save_img("fake.png", adv_image[0])
```

To load this image, use Pillow.

```python
from PIL import Image

fake_img = Image.open("fake.png")
fake_img
```

<br />

